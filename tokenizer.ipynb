{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "30df648f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते, आप कैसे हैं (hello how are you)'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"नमस्ते, आप कैसे हैं (hello how are you)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6eb057eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2344,\n",
       " 2350,\n",
       " 2360,\n",
       " 2381,\n",
       " 2340,\n",
       " 2375,\n",
       " 44,\n",
       " 32,\n",
       " 2310,\n",
       " 2346,\n",
       " 32,\n",
       " 2325,\n",
       " 2376,\n",
       " 2360,\n",
       " 2375,\n",
       " 32,\n",
       " 2361,\n",
       " 2376,\n",
       " 2306,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 104,\n",
       " 111,\n",
       " 119,\n",
       " 32,\n",
       " 97,\n",
       " 114,\n",
       " 101,\n",
       " 32,\n",
       " 121,\n",
       " 111,\n",
       " 117,\n",
       " 41]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"नमस्ते, आप कैसे हैं (hello how are you)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c3fc1b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[224,\n",
       " 164,\n",
       " 168,\n",
       " 224,\n",
       " 164,\n",
       " 174,\n",
       " 224,\n",
       " 164,\n",
       " 184,\n",
       " 224,\n",
       " 165,\n",
       " 141,\n",
       " 224,\n",
       " 164,\n",
       " 164,\n",
       " 224,\n",
       " 165,\n",
       " 135,\n",
       " 44,\n",
       " 32,\n",
       " 224,\n",
       " 164,\n",
       " 134,\n",
       " 224,\n",
       " 164,\n",
       " 170,\n",
       " 32,\n",
       " 224,\n",
       " 164,\n",
       " 149,\n",
       " 224,\n",
       " 165,\n",
       " 136,\n",
       " 224,\n",
       " 164,\n",
       " 184,\n",
       " 224,\n",
       " 165,\n",
       " 135,\n",
       " 32,\n",
       " 224,\n",
       " 164,\n",
       " 185,\n",
       " 224,\n",
       " 165,\n",
       " 136,\n",
       " 224,\n",
       " 164,\n",
       " 130,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 104,\n",
       " 111,\n",
       " 119,\n",
       " 32,\n",
       " 97,\n",
       " 114,\n",
       " 101,\n",
       " 32,\n",
       " 121,\n",
       " 111,\n",
       " 117,\n",
       " 41]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"नमस्ते, आप कैसे हैं (hello how are you)\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e748ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "are marked by a ⊛ in the name and outlined images; their images may show as a group with “…” before and after. OpenAI's large language models process text using tokens, which are common sequences of characters found in a set of text. The models learn to understand the statistical relationships between these tokens, and excel at producing the next token in a sequence of tokens. Learn more.  You can use the tool below to understand how a piece of text might be tokenized by a language model, and the total count of tokens in that piece of text.\n",
      "length :  546\n",
      "----\n",
      "[97, 114, 101, 32, 109, 97, 114, 107, 101, 100, 32, 98, 121, 32, 97, 32, 226, 138, 155, 32, 105, 110, 32, 116, 104, 101, 32, 110, 97, 109, 101, 32, 97, 110, 100, 32, 111, 117, 116, 108, 105, 110, 101, 100, 32, 105, 109, 97, 103, 101, 115, 59, 32, 116, 104, 101, 105, 114, 32, 105, 109, 97, 103, 101, 115, 32, 109, 97, 121, 32, 115, 104, 111, 119, 32, 97, 115, 32, 97, 32, 103, 114, 111, 117, 112, 32, 119, 105, 116, 104, 32, 226, 128, 156, 226, 128, 166, 226, 128, 157, 32, 98, 101, 102, 111, 114, 101, 32, 97, 110, 100, 32, 97, 102, 116, 101, 114, 46, 32, 79, 112, 101, 110, 65, 73, 39, 115, 32, 108, 97, 114, 103, 101, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 115, 32, 112, 114, 111, 99, 101, 115, 115, 32, 116, 101, 120, 116, 32, 117, 115, 105, 110, 103, 32, 116, 111, 107, 101, 110, 115, 44, 32, 119, 104, 105, 99, 104, 32, 97, 114, 101, 32, 99, 111, 109, 109, 111, 110, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 111, 102, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 102, 111, 117, 110, 100, 32, 105, 110, 32, 97, 32, 115, 101, 116, 32, 111, 102, 32, 116, 101, 120, 116, 46, 32, 84, 104, 101, 32, 109, 111, 100, 101, 108, 115, 32, 108, 101, 97, 114, 110, 32, 116, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 32, 116, 104, 101, 32, 115, 116, 97, 116, 105, 115, 116, 105, 99, 97, 108, 32, 114, 101, 108, 97, 116, 105, 111, 110, 115, 104, 105, 112, 115, 32, 98, 101, 116, 119, 101, 101, 110, 32, 116, 104, 101, 115, 101, 32, 116, 111, 107, 101, 110, 115, 44, 32, 97, 110, 100, 32, 101, 120, 99, 101, 108, 32, 97, 116, 32, 112, 114, 111, 100, 117, 99, 105, 110, 103, 32, 116, 104, 101, 32, 110, 101, 120, 116, 32, 116, 111, 107, 101, 110, 32, 105, 110, 32, 97, 32, 115, 101, 113, 117, 101, 110, 99, 101, 32, 111, 102, 32, 116, 111, 107, 101, 110, 115, 46, 32, 76, 101, 97, 114, 110, 32, 109, 111, 114, 101, 46, 32, 32, 89, 111, 117, 32, 99, 97, 110, 32, 117, 115, 101, 32, 116, 104, 101, 32, 116, 111, 111, 108, 32, 98, 101, 108, 111, 119, 32, 116, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 32, 104, 111, 119, 32, 97, 32, 112, 105, 101, 99, 101, 32, 111, 102, 32, 116, 101, 120, 116, 32, 109, 105, 103, 104, 116, 32, 98, 101, 32, 116, 111, 107, 101, 110, 105, 122, 101, 100, 32, 98, 121, 32, 97, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 44, 32, 97, 110, 100, 32, 116, 104, 101, 32, 116, 111, 116, 97, 108, 32, 99, 111, 117, 110, 116, 32, 111, 102, 32, 116, 111, 107, 101, 110, 115, 32, 105, 110, 32, 116, 104, 97, 116, 32, 112, 105, 101, 99, 101, 32, 111, 102, 32, 116, 101, 120, 116, 46]\n",
      "length :  554\n"
     ]
    }
   ],
   "source": [
    "text = \"are marked by a ⊛ in the name and outlined images; their images may show as a group with “…” before and after. OpenAI's large language models process text using tokens, which are common sequences of characters found in a set of text. The models learn to understand the statistical relationships between these tokens, and excel at producing the next token in a sequence of tokens. Learn more.  You can use the tool below to understand how a piece of text might be tokenized by a language model, and the total count of tokens in that piece of text.\"\n",
    "tokens = text.encode('utf-8') # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0...255 for conveience\n",
    "print('----')\n",
    "print(text)\n",
    "print(\"length : \",len(text))\n",
    "print('----')\n",
    "print(tokens)\n",
    "print(\"length : \",len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "07aa9dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t', 'o')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(116), chr(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5b4ec05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(97, 114): 7, (114, 101): 5, (101, 32): 19, (32, 109): 7, (109, 97): 4, (114, 107): 1, (107, 101): 7, (101, 100): 3, (100, 32): 10, (32, 98): 6, (98, 121): 2, (121, 32): 3, (32, 97): 14, (97, 32): 6, (32, 226): 2, (226, 138): 1, (138, 155): 1, (155, 32): 1, (32, 105): 6, (105, 110): 7, (110, 32): 10, (32, 116): 22, (116, 104): 9, (104, 101): 8, (32, 110): 2, (110, 97): 1, (97, 109): 1, (109, 101): 1, (97, 110): 9, (110, 100): 9, (32, 111): 7, (111, 117): 5, (117, 116): 1, (116, 108): 1, (108, 105): 1, (110, 101): 2, (105, 109): 2, (97, 103): 4, (103, 101): 5, (101, 115): 5, (115, 59): 1, (59, 32): 1, (101, 105): 1, (105, 114): 1, (114, 32): 1, (115, 32): 10, (97, 121): 1, (32, 115): 5, (115, 104): 2, (104, 111): 2, (111, 119): 3, (119, 32): 3, (97, 115): 1, (32, 103): 1, (103, 114): 1, (114, 111): 3, (117, 112): 1, (112, 32): 1, (32, 119): 2, (119, 105): 1, (105, 116): 1, (104, 32): 2, (226, 128): 3, (128, 156): 1, (156, 226): 1, (128, 166): 1, (166, 226): 1, (128, 157): 1, (157, 32): 1, (98, 101): 4, (101, 102): 1, (102, 111): 2, (111, 114): 2, (97, 102): 1, (102, 116): 1, (116, 101): 6, (101, 114): 4, (114, 46): 1, (46, 32): 4, (32, 79): 1, (79, 112): 1, (112, 101): 1, (101, 110): 10, (110, 65): 1, (65, 73): 1, (73, 39): 1, (39, 115): 1, (32, 108): 4, (108, 97): 4, (114, 103): 1, (110, 103): 4, (103, 117): 2, (117, 97): 2, (109, 111): 5, (111, 100): 4, (100, 101): 5, (101, 108): 6, (108, 115): 2, (32, 112): 4, (112, 114): 2, (111, 99): 1, (99, 101): 6, (115, 115): 1, (101, 120): 6, (120, 116): 5, (116, 32): 8, (32, 117): 4, (117, 115): 2, (115, 105): 1, (103, 32): 2, (116, 111): 10, (111, 107): 6, (110, 115): 5, (115, 44): 2, (44, 32): 3, (119, 104): 1, (104, 105): 2, (105, 99): 2, (99, 104): 2, (32, 99): 4, (99, 111): 2, (111, 109): 1, (109, 109): 1, (111, 110): 2, (115, 101): 5, (101, 113): 2, (113, 117): 2, (117, 101): 2, (110, 99): 2, (111, 102): 6, (102, 32): 6, (104, 97): 2, (114, 97): 1, (97, 99): 1, (99, 116): 1, (114, 115): 3, (32, 102): 1, (117, 110): 4, (101, 116): 2, (116, 46): 2, (32, 84): 1, (84, 104): 1, (108, 101): 1, (101, 97): 2, (114, 110): 2, (111, 32): 2, (115, 116): 4, (116, 97): 4, (97, 116): 4, (116, 105): 3, (105, 115): 1, (99, 97): 2, (97, 108): 2, (108, 32): 4, (32, 114): 1, (105, 111): 1, (105, 112): 1, (112, 115): 1, (116, 119): 1, (119, 101): 1, (101, 101): 1, (32, 101): 1, (120, 99): 1, (100, 117): 1, (117, 99): 1, (99, 105): 1, (115, 46): 1, (32, 76): 1, (76, 101): 1, (101, 46): 1, (32, 32): 1, (32, 89): 1, (89, 111): 1, (117, 32): 1, (111, 111): 1, (111, 108): 1, (108, 111): 1, (32, 104): 1, (112, 105): 2, (105, 101): 2, (101, 99): 2, (109, 105): 1, (105, 103): 1, (103, 104): 1, (104, 116): 1, (110, 105): 1, (105, 122): 1, (122, 101): 1, (108, 44): 1, (111, 116): 1, (110, 116): 1}\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(stats)\n",
    "\n",
    "# print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "55d90458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 116)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d4284600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 114, 101, 32, 109, 97, 114, 107, 101, 100, 32, 98, 121, 32, 97, 32, 226, 138, 155, 32, 105, 110, 256, 104, 101, 32, 110, 97, 109, 101, 32, 97, 110, 100, 32, 111, 117, 116, 108, 105, 110, 101, 100, 32, 105, 109, 97, 103, 101, 115, 59, 256, 104, 101, 105, 114, 32, 105, 109, 97, 103, 101, 115, 32, 109, 97, 121, 32, 115, 104, 111, 119, 32, 97, 115, 32, 97, 32, 103, 114, 111, 117, 112, 32, 119, 105, 116, 104, 32, 226, 128, 156, 226, 128, 166, 226, 128, 157, 32, 98, 101, 102, 111, 114, 101, 32, 97, 110, 100, 32, 97, 102, 116, 101, 114, 46, 32, 79, 112, 101, 110, 65, 73, 39, 115, 32, 108, 97, 114, 103, 101, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 115, 32, 112, 114, 111, 99, 101, 115, 115, 256, 101, 120, 116, 32, 117, 115, 105, 110, 103, 256, 111, 107, 101, 110, 115, 44, 32, 119, 104, 105, 99, 104, 32, 97, 114, 101, 32, 99, 111, 109, 109, 111, 110, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 111, 102, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 102, 111, 117, 110, 100, 32, 105, 110, 32, 97, 32, 115, 101, 116, 32, 111, 102, 256, 101, 120, 116, 46, 32, 84, 104, 101, 32, 109, 111, 100, 101, 108, 115, 32, 108, 101, 97, 114, 110, 256, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 256, 104, 101, 32, 115, 116, 97, 116, 105, 115, 116, 105, 99, 97, 108, 32, 114, 101, 108, 97, 116, 105, 111, 110, 115, 104, 105, 112, 115, 32, 98, 101, 116, 119, 101, 101, 110, 256, 104, 101, 115, 101, 256, 111, 107, 101, 110, 115, 44, 32, 97, 110, 100, 32, 101, 120, 99, 101, 108, 32, 97, 116, 32, 112, 114, 111, 100, 117, 99, 105, 110, 103, 256, 104, 101, 32, 110, 101, 120, 116, 256, 111, 107, 101, 110, 32, 105, 110, 32, 97, 32, 115, 101, 113, 117, 101, 110, 99, 101, 32, 111, 102, 256, 111, 107, 101, 110, 115, 46, 32, 76, 101, 97, 114, 110, 32, 109, 111, 114, 101, 46, 32, 32, 89, 111, 117, 32, 99, 97, 110, 32, 117, 115, 101, 256, 104, 101, 256, 111, 111, 108, 32, 98, 101, 108, 111, 119, 256, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 32, 104, 111, 119, 32, 97, 32, 112, 105, 101, 99, 101, 32, 111, 102, 256, 101, 120, 116, 32, 109, 105, 103, 104, 116, 32, 98, 101, 256, 111, 107, 101, 110, 105, 122, 101, 100, 32, 98, 121, 32, 97, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 44, 32, 97, 110, 100, 256, 104, 101, 256, 111, 116, 97, 108, 32, 99, 111, 117, 110, 116, 32, 111, 102, 256, 111, 107, 101, 110, 115, 32, 105, 110, 256, 104, 97, 116, 32, 112, 105, 101, 99, 101, 32, 111, 102, 256, 101, 120, 116, 46]\n",
      "length :  532\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # in the listof ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i<len(ids):\n",
    "        #if we are not at the very last position AND the pair matches, replace it\n",
    "        if i<len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# print(merge([1,2,3,2,3,4], (2,3), 99))  # expect [1,99,99,4]\n",
    "tokens = merge(tokens, top_pair, 256)\n",
    "print(tokens)\n",
    "print(\"length : \",len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cbe05e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (256, 111) into a new token 257\n",
      "merging (32, 97) into a new token 258\n",
      "merging (101, 110) into a new token 259\n",
      "merging (110, 100) into a new token 260\n",
      "merging (256, 104) into a new token 261\n",
      "merging (115, 32) into a new token 262\n",
      "merging (105, 110) into a new token 263\n",
      "merging (116, 32) into a new token 264\n",
      "merging (97, 114) into a new token 265\n",
      "merging (258, 32) into a new token 266\n",
      "merging (101, 108) into a new token 267\n",
      "merging (101, 120) into a new token 268\n",
      "merging (257, 107) into a new token 269\n",
      "merging (269, 259) into a new token 270\n",
      "merging (111, 117) into a new token 271\n",
      "merging (109, 111) into a new token 272\n",
      "merging (115, 101) into a new token 273\n",
      "merging (32, 98) into a new token 274\n",
      "merging (97, 260) into a new token 275\n",
      "merging (97, 103) into a new token 276\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 277\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)  # make a copy\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e3165482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 532\n",
      "ids length: 381\n",
      "compression ratio: 1.40X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\",len(tokens))\n",
    "print(\"ids length:\",len(ids))\n",
    "print(f\"compression ratio: {len(tokens)/len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc514ec",
   "metadata": {},
   "source": [
    "# Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a60282ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for  idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    # give ids (list of integers), return Python String\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6746df67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (256, 111): 257,\n",
       " (32, 97): 258,\n",
       " (101, 110): 259,\n",
       " (110, 100): 260,\n",
       " (256, 104): 261,\n",
       " (115, 32): 262,\n",
       " (105, 110): 263,\n",
       " (116, 32): 264,\n",
       " (97, 114): 265,\n",
       " (258, 32): 266,\n",
       " (101, 108): 267,\n",
       " (101, 120): 268,\n",
       " (257, 107): 269,\n",
       " (269, 259): 270,\n",
       " (111, 117): 271,\n",
       " (109, 111): 272,\n",
       " (115, 101): 273,\n",
       " (32, 98): 274,\n",
       " (97, 260): 275,\n",
       " (97, 103): 276}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ac9ba",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "49946bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # give a string, return list of integers (the tokens)\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break # noting else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "print(encode(\"h\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0e7ddbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you?\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello how are you?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9fbca9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea96a6e5",
   "metadata": {},
   "source": [
    "# Force splits using regex patterns (GPT series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "571851ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', '       ', ' you', '!?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat,\"hello've world123 how's are        you!?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4401a935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'fruits', ' =', ' [\"', 'apple', '\",', ' \"', 'banana', '\",', ' \"', 'cherry', '\"]', '\\n', 'for', ' x', ' in', ' fruits', ':', '\\n ', ' print', '(', 'x', ')', '\\n ', ' if', ' x', ' ==', ' \"', 'banana', '\":', '\\n   ', ' break', '\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
    "for x in fruits:\n",
    "  print(x)\n",
    "  if x == \"banana\":\n",
    "    break\n",
    "\"\"\"\n",
    "\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "374f3572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185]\n",
      "[262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "#GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8accb894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6962e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"-12abcd\"\n",
    "# a=text.encode(\"utf-8\")\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "461441a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45, 49, 50, 97, 98, 99, 100]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b6fc979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 114, 101, 32, 109, 97, 114, 107, 101, 100, 32, 98, 121, 32, 97, 32, 226, 138, 155, 32, 105, 110, 32, 116, 104, 101, 32, 110, 97, 109, 101, 32, 97, 110, 100, 32, 111, 117, 116, 108, 105, 110, 101, 100, 32, 105, 109, 97, 103, 101, 115, 59, 32, 116, 104, 101, 105, 114, 32, 105, 109, 97, 103, 101, 115, 32, 109, 97, 121, 32, 115, 104, 111, 119, 32, 97, 115, 32, 97, 32, 103, 114, 111, 117, 112, 32, 119, 105, 116, 104, 32, 226, 128, 156, 226, 128, 166, 226, 128, 157, 32, 98, 101, 102, 111, 114, 101, 32, 97, 110, 100, 32, 97, 102, 116, 101, 114, 46, 32, 79, 112, 101, 110, 65, 73, 39, 115, 32, 108, 97, 114, 103, 101, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 115, 32, 112, 114, 111, 99, 101, 115, 115, 32, 116, 101, 120, 116, 32, 117, 115, 105, 110, 103, 32, 116, 111, 107, 101, 110, 115, 44, 32, 119, 104, 105, 99, 104, 32, 97, 114, 101, 32, 99, 111, 109, 109, 111, 110, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 111, 102, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 102, 111, 117, 110, 100, 32, 105, 110, 32, 97, 32, 115, 101, 116, 32, 111, 102, 32, 116, 101, 120, 116, 46, 32, 84, 104, 101, 32, 109, 111, 100, 101, 108, 115, 32, 108, 101, 97, 114, 110, 32, 116, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 32, 116, 104, 101, 32, 115, 116, 97, 116, 105, 115, 116, 105, 99, 97, 108, 32, 114, 101, 108, 97, 116, 105, 111, 110, 115, 104, 105, 112, 115, 32, 98, 101, 116, 119, 101, 101, 110, 32, 116, 104, 101, 115, 101, 32, 116, 111, 107, 101, 110, 115, 44, 32, 97, 110, 100, 32, 101, 120, 99, 101, 108, 32, 97, 116, 32, 112, 114, 111, 100, 117, 99, 105, 110, 103, 32, 116, 104, 101, 32, 110, 101, 120, 116, 32, 116, 111, 107, 101, 110, 32, 105, 110, 32, 97, 32, 115, 101, 113, 117, 101, 110, 99, 101, 32, 111, 102, 32, 116, 111, 107, 101, 110, 115, 46, 32, 76, 101, 97, 114, 110, 32, 109, 111, 114, 101, 46, 32, 32, 89, 111, 117, 32, 99, 97, 110, 32, 117, 115, 101, 32, 116, 104, 101, 32, 116, 111, 111, 108, 32, 98, 101, 108, 111, 119, 32, 116, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 32, 104, 111, 119, 32, 97, 32, 112, 105, 101, 99, 101, 32, 111, 102, 32, 116, 101, 120, 116, 32, 109, 105, 103, 104, 116, 32, 98, 101, 32, 116, 111, 107, 101, 110, 105, 122, 101, 100, 32, 98, 121, 32, 97, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 44, 32, 97, 110, 100, 32, 116, 104, 101, 32, 116, 111, 116, 97, 108, 32, 99, 111, 117, 110, 116, 32, 111, 102, 32, 116, 111, 107, 101, 110, 115, 32, 105, 110, 32, 116, 104, 97, 116, 32, 112, 105, 101, 99, 101, 32, 111, 102, 32, 116, 101, 120, 116, 46]\n"
     ]
    }
   ],
   "source": [
    "from tokenizer.basic import BasicTokenizer\n",
    "t = BasicTokenizer()\n",
    "text1 = \"aaabdaaabac\"\n",
    "t.train(text1, 256 + 3)\n",
    "print(t.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a72ee511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1st: aaab \n",
      " 2nd: d \n",
      " 3rd: a\n"
     ]
    }
   ],
   "source": [
    "print(f\" 1st: {t.decode([258])} \\n 2nd: {t.decode([100])} \\n 3rd: {t.decode([97])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9cc289d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "compile_pattern = re.compile(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d1fe9898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are',\n",
       " ' marked',\n",
       " ' by',\n",
       " ' a',\n",
       " ' ⊛',\n",
       " ' in',\n",
       " ' the',\n",
       " ' name',\n",
       " ' and',\n",
       " ' outlined',\n",
       " ' images',\n",
       " ';',\n",
       " ' their',\n",
       " ' images',\n",
       " ' may',\n",
       " ' show',\n",
       " ' as',\n",
       " ' a',\n",
       " ' group',\n",
       " ' with',\n",
       " ' “…”',\n",
       " ' before',\n",
       " ' and',\n",
       " ' after',\n",
       " '.',\n",
       " ' OpenAI',\n",
       " \"'s\",\n",
       " ' large',\n",
       " ' language',\n",
       " ' models',\n",
       " ' process',\n",
       " ' text',\n",
       " ' using',\n",
       " ' tokens',\n",
       " ',',\n",
       " ' which',\n",
       " ' are',\n",
       " ' common',\n",
       " ' sequences',\n",
       " ' of',\n",
       " ' characters',\n",
       " ' found',\n",
       " ' in',\n",
       " ' a',\n",
       " ' set',\n",
       " ' of',\n",
       " ' text',\n",
       " '.',\n",
       " ' The',\n",
       " ' models',\n",
       " ' learn',\n",
       " ' to',\n",
       " ' understand',\n",
       " ' the',\n",
       " ' statistical',\n",
       " ' relationships',\n",
       " ' between',\n",
       " ' these',\n",
       " ' tokens',\n",
       " ',',\n",
       " ' and',\n",
       " ' excel',\n",
       " ' at',\n",
       " ' producing',\n",
       " ' the',\n",
       " ' next',\n",
       " ' token',\n",
       " ' in',\n",
       " ' a',\n",
       " ' sequence',\n",
       " ' of',\n",
       " ' tokens',\n",
       " '.',\n",
       " ' Learn',\n",
       " ' more',\n",
       " '.',\n",
       " ' ',\n",
       " ' You',\n",
       " ' can',\n",
       " ' use',\n",
       " ' the',\n",
       " ' tool',\n",
       " ' below',\n",
       " ' to',\n",
       " ' understand',\n",
       " ' how',\n",
       " ' a',\n",
       " ' piece',\n",
       " ' of',\n",
       " ' text',\n",
       " ' might',\n",
       " ' be',\n",
       " ' tokenized',\n",
       " ' by',\n",
       " ' a',\n",
       " ' language',\n",
       " ' model',\n",
       " ',',\n",
       " ' and',\n",
       " ' the',\n",
       " ' total',\n",
       " ' count',\n",
       " ' of',\n",
       " ' tokens',\n",
       " ' in',\n",
       " ' that',\n",
       " ' piece',\n",
       " ' of',\n",
       " ' text',\n",
       " '.']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = re.findall(compile_pattern, text)\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e1826b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.decode([97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2ca9032b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[97, 114, 101],\n",
       " [32, 109, 97, 114, 107, 101, 100],\n",
       " [32, 98, 121],\n",
       " [32, 97],\n",
       " [32, 226, 138, 155],\n",
       " [32, 105, 110],\n",
       " [32, 116, 104, 101],\n",
       " [32, 110, 97, 109, 101],\n",
       " [32, 97, 110, 100],\n",
       " [32, 111, 117, 116, 108, 105, 110, 101, 100],\n",
       " [32, 105, 109, 97, 103, 101, 115],\n",
       " [59],\n",
       " [32, 116, 104, 101, 105, 114],\n",
       " [32, 105, 109, 97, 103, 101, 115],\n",
       " [32, 109, 97, 121],\n",
       " [32, 115, 104, 111, 119],\n",
       " [32, 97, 115],\n",
       " [32, 97],\n",
       " [32, 103, 114, 111, 117, 112],\n",
       " [32, 119, 105, 116, 104],\n",
       " [32, 226, 128, 156, 226, 128, 166, 226, 128, 157],\n",
       " [32, 98, 101, 102, 111, 114, 101],\n",
       " [32, 97, 110, 100],\n",
       " [32, 97, 102, 116, 101, 114],\n",
       " [46],\n",
       " [32, 79, 112, 101, 110, 65, 73],\n",
       " [39, 115],\n",
       " [32, 108, 97, 114, 103, 101],\n",
       " [32, 108, 97, 110, 103, 117, 97, 103, 101],\n",
       " [32, 109, 111, 100, 101, 108, 115],\n",
       " [32, 112, 114, 111, 99, 101, 115, 115],\n",
       " [32, 116, 101, 120, 116],\n",
       " [32, 117, 115, 105, 110, 103],\n",
       " [32, 116, 111, 107, 101, 110, 115],\n",
       " [44],\n",
       " [32, 119, 104, 105, 99, 104],\n",
       " [32, 97, 114, 101],\n",
       " [32, 99, 111, 109, 109, 111, 110],\n",
       " [32, 115, 101, 113, 117, 101, 110, 99, 101, 115],\n",
       " [32, 111, 102],\n",
       " [32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115],\n",
       " [32, 102, 111, 117, 110, 100],\n",
       " [32, 105, 110],\n",
       " [32, 97],\n",
       " [32, 115, 101, 116],\n",
       " [32, 111, 102],\n",
       " [32, 116, 101, 120, 116],\n",
       " [46],\n",
       " [32, 84, 104, 101],\n",
       " [32, 109, 111, 100, 101, 108, 115],\n",
       " [32, 108, 101, 97, 114, 110],\n",
       " [32, 116, 111],\n",
       " [32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100],\n",
       " [32, 116, 104, 101],\n",
       " [32, 115, 116, 97, 116, 105, 115, 116, 105, 99, 97, 108],\n",
       " [32, 114, 101, 108, 97, 116, 105, 111, 110, 115, 104, 105, 112, 115],\n",
       " [32, 98, 101, 116, 119, 101, 101, 110],\n",
       " [32, 116, 104, 101, 115, 101],\n",
       " [32, 116, 111, 107, 101, 110, 115],\n",
       " [44],\n",
       " [32, 97, 110, 100],\n",
       " [32, 101, 120, 99, 101, 108],\n",
       " [32, 97, 116],\n",
       " [32, 112, 114, 111, 100, 117, 99, 105, 110, 103],\n",
       " [32, 116, 104, 101],\n",
       " [32, 110, 101, 120, 116],\n",
       " [32, 116, 111, 107, 101, 110],\n",
       " [32, 105, 110],\n",
       " [32, 97],\n",
       " [32, 115, 101, 113, 117, 101, 110, 99, 101],\n",
       " [32, 111, 102],\n",
       " [32, 116, 111, 107, 101, 110, 115],\n",
       " [46],\n",
       " [32, 76, 101, 97, 114, 110],\n",
       " [32, 109, 111, 114, 101],\n",
       " [46],\n",
       " [32],\n",
       " [32, 89, 111, 117],\n",
       " [32, 99, 97, 110],\n",
       " [32, 117, 115, 101],\n",
       " [32, 116, 104, 101],\n",
       " [32, 116, 111, 111, 108],\n",
       " [32, 98, 101, 108, 111, 119],\n",
       " [32, 116, 111],\n",
       " [32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100],\n",
       " [32, 104, 111, 119],\n",
       " [32, 97],\n",
       " [32, 112, 105, 101, 99, 101],\n",
       " [32, 111, 102],\n",
       " [32, 116, 101, 120, 116],\n",
       " [32, 109, 105, 103, 104, 116],\n",
       " [32, 98, 101],\n",
       " [32, 116, 111, 107, 101, 110, 105, 122, 101, 100],\n",
       " [32, 98, 121],\n",
       " [32, 97],\n",
       " [32, 108, 97, 110, 103, 117, 97, 103, 101],\n",
       " [32, 109, 111, 100, 101, 108],\n",
       " [44],\n",
       " [32, 97, 110, 100],\n",
       " [32, 116, 104, 101],\n",
       " [32, 116, 111, 116, 97, 108],\n",
       " [32, 99, 111, 117, 110, 116],\n",
       " [32, 111, 102],\n",
       " [32, 116, 111, 107, 101, 110, 115],\n",
       " [32, 105, 110],\n",
       " [32, 116, 104, 97, 116],\n",
       " [32, 112, 105, 101, 99, 101],\n",
       " [32, 111, 102],\n",
       " [32, 116, 101, 120, 116],\n",
       " [46]]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids =[list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
