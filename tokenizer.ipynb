{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "30df648f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते, आप कैसे हैं (hello how are you)'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"नमस्ते, आप कैसे हैं (hello how are you)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6eb057eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2344,\n",
       " 2350,\n",
       " 2360,\n",
       " 2381,\n",
       " 2340,\n",
       " 2375,\n",
       " 44,\n",
       " 32,\n",
       " 2310,\n",
       " 2346,\n",
       " 32,\n",
       " 2325,\n",
       " 2376,\n",
       " 2360,\n",
       " 2375,\n",
       " 32,\n",
       " 2361,\n",
       " 2376,\n",
       " 2306,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 104,\n",
       " 111,\n",
       " 119,\n",
       " 32,\n",
       " 97,\n",
       " 114,\n",
       " 101,\n",
       " 32,\n",
       " 121,\n",
       " 111,\n",
       " 117,\n",
       " 41]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"नमस्ते, आप कैसे हैं (hello how are you)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c3fc1b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[224,\n",
       " 164,\n",
       " 168,\n",
       " 224,\n",
       " 164,\n",
       " 174,\n",
       " 224,\n",
       " 164,\n",
       " 184,\n",
       " 224,\n",
       " 165,\n",
       " 141,\n",
       " 224,\n",
       " 164,\n",
       " 164,\n",
       " 224,\n",
       " 165,\n",
       " 135,\n",
       " 44,\n",
       " 32,\n",
       " 224,\n",
       " 164,\n",
       " 134,\n",
       " 224,\n",
       " 164,\n",
       " 170,\n",
       " 32,\n",
       " 224,\n",
       " 164,\n",
       " 149,\n",
       " 224,\n",
       " 165,\n",
       " 136,\n",
       " 224,\n",
       " 164,\n",
       " 184,\n",
       " 224,\n",
       " 165,\n",
       " 135,\n",
       " 32,\n",
       " 224,\n",
       " 164,\n",
       " 185,\n",
       " 224,\n",
       " 165,\n",
       " 136,\n",
       " 224,\n",
       " 164,\n",
       " 130,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 104,\n",
       " 111,\n",
       " 119,\n",
       " 32,\n",
       " 97,\n",
       " 114,\n",
       " 101,\n",
       " 32,\n",
       " 121,\n",
       " 111,\n",
       " 117,\n",
       " 41]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"नमस्ते, आप कैसे हैं (hello how are you)\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e748ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "are marked by a ⊛ in the name and outlined images; their images may show as a group with “…” before and after. OpenAI's large language models process text using tokens, which are common sequences of characters found in a set of text. The models learn to understand the statistical relationships between these tokens, and excel at producing the next token in a sequence of tokens. Learn more.  You can use the tool below to understand how a piece of text might be tokenized by a language model, and the total count of tokens in that piece of text.\n",
      "length :  546\n",
      "----\n",
      "[97, 114, 101, 32, 109, 97, 114, 107, 101, 100, 32, 98, 121, 32, 97, 32, 226, 138, 155, 32, 105, 110, 32, 116, 104, 101, 32, 110, 97, 109, 101, 32, 97, 110, 100, 32, 111, 117, 116, 108, 105, 110, 101, 100, 32, 105, 109, 97, 103, 101, 115, 59, 32, 116, 104, 101, 105, 114, 32, 105, 109, 97, 103, 101, 115, 32, 109, 97, 121, 32, 115, 104, 111, 119, 32, 97, 115, 32, 97, 32, 103, 114, 111, 117, 112, 32, 119, 105, 116, 104, 32, 226, 128, 156, 226, 128, 166, 226, 128, 157, 32, 98, 101, 102, 111, 114, 101, 32, 97, 110, 100, 32, 97, 102, 116, 101, 114, 46, 32, 79, 112, 101, 110, 65, 73, 39, 115, 32, 108, 97, 114, 103, 101, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 115, 32, 112, 114, 111, 99, 101, 115, 115, 32, 116, 101, 120, 116, 32, 117, 115, 105, 110, 103, 32, 116, 111, 107, 101, 110, 115, 44, 32, 119, 104, 105, 99, 104, 32, 97, 114, 101, 32, 99, 111, 109, 109, 111, 110, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 111, 102, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 102, 111, 117, 110, 100, 32, 105, 110, 32, 97, 32, 115, 101, 116, 32, 111, 102, 32, 116, 101, 120, 116, 46, 32, 84, 104, 101, 32, 109, 111, 100, 101, 108, 115, 32, 108, 101, 97, 114, 110, 32, 116, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 32, 116, 104, 101, 32, 115, 116, 97, 116, 105, 115, 116, 105, 99, 97, 108, 32, 114, 101, 108, 97, 116, 105, 111, 110, 115, 104, 105, 112, 115, 32, 98, 101, 116, 119, 101, 101, 110, 32, 116, 104, 101, 115, 101, 32, 116, 111, 107, 101, 110, 115, 44, 32, 97, 110, 100, 32, 101, 120, 99, 101, 108, 32, 97, 116, 32, 112, 114, 111, 100, 117, 99, 105, 110, 103, 32, 116, 104, 101, 32, 110, 101, 120, 116, 32, 116, 111, 107, 101, 110, 32, 105, 110, 32, 97, 32, 115, 101, 113, 117, 101, 110, 99, 101, 32, 111, 102, 32, 116, 111, 107, 101, 110, 115, 46, 32, 76, 101, 97, 114, 110, 32, 109, 111, 114, 101, 46, 32, 32, 89, 111, 117, 32, 99, 97, 110, 32, 117, 115, 101, 32, 116, 104, 101, 32, 116, 111, 111, 108, 32, 98, 101, 108, 111, 119, 32, 116, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 32, 104, 111, 119, 32, 97, 32, 112, 105, 101, 99, 101, 32, 111, 102, 32, 116, 101, 120, 116, 32, 109, 105, 103, 104, 116, 32, 98, 101, 32, 116, 111, 107, 101, 110, 105, 122, 101, 100, 32, 98, 121, 32, 97, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 44, 32, 97, 110, 100, 32, 116, 104, 101, 32, 116, 111, 116, 97, 108, 32, 99, 111, 117, 110, 116, 32, 111, 102, 32, 116, 111, 107, 101, 110, 115, 32, 105, 110, 32, 116, 104, 97, 116, 32, 112, 105, 101, 99, 101, 32, 111, 102, 32, 116, 101, 120, 116, 46]\n",
      "length :  554\n"
     ]
    }
   ],
   "source": [
    "text = \"are marked by a ⊛ in the name and outlined images; their images may show as a group with “…” before and after. OpenAI's large language models process text using tokens, which are common sequences of characters found in a set of text. The models learn to understand the statistical relationships between these tokens, and excel at producing the next token in a sequence of tokens. Learn more.  You can use the tool below to understand how a piece of text might be tokenized by a language model, and the total count of tokens in that piece of text.\"\n",
    "tokens = text.encode('utf-8') # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0...255 for conveience\n",
    "print('----')\n",
    "print(text)\n",
    "print(\"length : \",len(text))\n",
    "print('----')\n",
    "print(tokens)\n",
    "print(\"length : \",len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "07aa9dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t', 'o')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(116), chr(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5b4ec05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(97, 114): 7, (114, 101): 5, (101, 32): 19, (32, 109): 7, (109, 97): 4, (114, 107): 1, (107, 101): 7, (101, 100): 3, (100, 32): 10, (32, 98): 6, (98, 121): 2, (121, 32): 3, (32, 97): 14, (97, 32): 6, (32, 226): 2, (226, 138): 1, (138, 155): 1, (155, 32): 1, (32, 105): 6, (105, 110): 7, (110, 32): 10, (32, 116): 22, (116, 104): 9, (104, 101): 8, (32, 110): 2, (110, 97): 1, (97, 109): 1, (109, 101): 1, (97, 110): 9, (110, 100): 9, (32, 111): 7, (111, 117): 5, (117, 116): 1, (116, 108): 1, (108, 105): 1, (110, 101): 2, (105, 109): 2, (97, 103): 4, (103, 101): 5, (101, 115): 5, (115, 59): 1, (59, 32): 1, (101, 105): 1, (105, 114): 1, (114, 32): 1, (115, 32): 10, (97, 121): 1, (32, 115): 5, (115, 104): 2, (104, 111): 2, (111, 119): 3, (119, 32): 3, (97, 115): 1, (32, 103): 1, (103, 114): 1, (114, 111): 3, (117, 112): 1, (112, 32): 1, (32, 119): 2, (119, 105): 1, (105, 116): 1, (104, 32): 2, (226, 128): 3, (128, 156): 1, (156, 226): 1, (128, 166): 1, (166, 226): 1, (128, 157): 1, (157, 32): 1, (98, 101): 4, (101, 102): 1, (102, 111): 2, (111, 114): 2, (97, 102): 1, (102, 116): 1, (116, 101): 6, (101, 114): 4, (114, 46): 1, (46, 32): 4, (32, 79): 1, (79, 112): 1, (112, 101): 1, (101, 110): 10, (110, 65): 1, (65, 73): 1, (73, 39): 1, (39, 115): 1, (32, 108): 4, (108, 97): 4, (114, 103): 1, (110, 103): 4, (103, 117): 2, (117, 97): 2, (109, 111): 5, (111, 100): 4, (100, 101): 5, (101, 108): 6, (108, 115): 2, (32, 112): 4, (112, 114): 2, (111, 99): 1, (99, 101): 6, (115, 115): 1, (101, 120): 6, (120, 116): 5, (116, 32): 8, (32, 117): 4, (117, 115): 2, (115, 105): 1, (103, 32): 2, (116, 111): 10, (111, 107): 6, (110, 115): 5, (115, 44): 2, (44, 32): 3, (119, 104): 1, (104, 105): 2, (105, 99): 2, (99, 104): 2, (32, 99): 4, (99, 111): 2, (111, 109): 1, (109, 109): 1, (111, 110): 2, (115, 101): 5, (101, 113): 2, (113, 117): 2, (117, 101): 2, (110, 99): 2, (111, 102): 6, (102, 32): 6, (104, 97): 2, (114, 97): 1, (97, 99): 1, (99, 116): 1, (114, 115): 3, (32, 102): 1, (117, 110): 4, (101, 116): 2, (116, 46): 2, (32, 84): 1, (84, 104): 1, (108, 101): 1, (101, 97): 2, (114, 110): 2, (111, 32): 2, (115, 116): 4, (116, 97): 4, (97, 116): 4, (116, 105): 3, (105, 115): 1, (99, 97): 2, (97, 108): 2, (108, 32): 4, (32, 114): 1, (105, 111): 1, (105, 112): 1, (112, 115): 1, (116, 119): 1, (119, 101): 1, (101, 101): 1, (32, 101): 1, (120, 99): 1, (100, 117): 1, (117, 99): 1, (99, 105): 1, (115, 46): 1, (32, 76): 1, (76, 101): 1, (101, 46): 1, (32, 32): 1, (32, 89): 1, (89, 111): 1, (117, 32): 1, (111, 111): 1, (111, 108): 1, (108, 111): 1, (32, 104): 1, (112, 105): 2, (105, 101): 2, (101, 99): 2, (109, 105): 1, (105, 103): 1, (103, 104): 1, (104, 116): 1, (110, 105): 1, (105, 122): 1, (122, 101): 1, (108, 44): 1, (111, 116): 1, (110, 116): 1}\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(stats)\n",
    "\n",
    "# print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "55d90458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 116)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d4284600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 114, 101, 32, 109, 97, 114, 107, 101, 100, 32, 98, 121, 32, 97, 32, 226, 138, 155, 32, 105, 110, 256, 104, 101, 32, 110, 97, 109, 101, 32, 97, 110, 100, 32, 111, 117, 116, 108, 105, 110, 101, 100, 32, 105, 109, 97, 103, 101, 115, 59, 256, 104, 101, 105, 114, 32, 105, 109, 97, 103, 101, 115, 32, 109, 97, 121, 32, 115, 104, 111, 119, 32, 97, 115, 32, 97, 32, 103, 114, 111, 117, 112, 32, 119, 105, 116, 104, 32, 226, 128, 156, 226, 128, 166, 226, 128, 157, 32, 98, 101, 102, 111, 114, 101, 32, 97, 110, 100, 32, 97, 102, 116, 101, 114, 46, 32, 79, 112, 101, 110, 65, 73, 39, 115, 32, 108, 97, 114, 103, 101, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 115, 32, 112, 114, 111, 99, 101, 115, 115, 256, 101, 120, 116, 32, 117, 115, 105, 110, 103, 256, 111, 107, 101, 110, 115, 44, 32, 119, 104, 105, 99, 104, 32, 97, 114, 101, 32, 99, 111, 109, 109, 111, 110, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 111, 102, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 102, 111, 117, 110, 100, 32, 105, 110, 32, 97, 32, 115, 101, 116, 32, 111, 102, 256, 101, 120, 116, 46, 32, 84, 104, 101, 32, 109, 111, 100, 101, 108, 115, 32, 108, 101, 97, 114, 110, 256, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 256, 104, 101, 32, 115, 116, 97, 116, 105, 115, 116, 105, 99, 97, 108, 32, 114, 101, 108, 97, 116, 105, 111, 110, 115, 104, 105, 112, 115, 32, 98, 101, 116, 119, 101, 101, 110, 256, 104, 101, 115, 101, 256, 111, 107, 101, 110, 115, 44, 32, 97, 110, 100, 32, 101, 120, 99, 101, 108, 32, 97, 116, 32, 112, 114, 111, 100, 117, 99, 105, 110, 103, 256, 104, 101, 32, 110, 101, 120, 116, 256, 111, 107, 101, 110, 32, 105, 110, 32, 97, 32, 115, 101, 113, 117, 101, 110, 99, 101, 32, 111, 102, 256, 111, 107, 101, 110, 115, 46, 32, 76, 101, 97, 114, 110, 32, 109, 111, 114, 101, 46, 32, 32, 89, 111, 117, 32, 99, 97, 110, 32, 117, 115, 101, 256, 104, 101, 256, 111, 111, 108, 32, 98, 101, 108, 111, 119, 256, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 32, 104, 111, 119, 32, 97, 32, 112, 105, 101, 99, 101, 32, 111, 102, 256, 101, 120, 116, 32, 109, 105, 103, 104, 116, 32, 98, 101, 256, 111, 107, 101, 110, 105, 122, 101, 100, 32, 98, 121, 32, 97, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 44, 32, 97, 110, 100, 256, 104, 101, 256, 111, 116, 97, 108, 32, 99, 111, 117, 110, 116, 32, 111, 102, 256, 111, 107, 101, 110, 115, 32, 105, 110, 256, 104, 97, 116, 32, 112, 105, 101, 99, 101, 32, 111, 102, 256, 101, 120, 116, 46]\n",
      "length :  532\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # in the listof ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i<len(ids):\n",
    "        #if we are not at the very last position AND the pair matches, replace it\n",
    "        if i<len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# print(merge([1,2,3,2,3,4], (2,3), 99))  # expect [1,99,99,4]\n",
    "tokens = merge(tokens, top_pair, 256)\n",
    "print(tokens)\n",
    "print(\"length : \",len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cbe05e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (256, 111) into a new token 257\n",
      "merging (32, 97) into a new token 258\n",
      "merging (101, 110) into a new token 259\n",
      "merging (110, 100) into a new token 260\n",
      "merging (256, 104) into a new token 261\n",
      "merging (115, 32) into a new token 262\n",
      "merging (105, 110) into a new token 263\n",
      "merging (116, 32) into a new token 264\n",
      "merging (97, 114) into a new token 265\n",
      "merging (258, 32) into a new token 266\n",
      "merging (101, 108) into a new token 267\n",
      "merging (101, 120) into a new token 268\n",
      "merging (257, 107) into a new token 269\n",
      "merging (269, 259) into a new token 270\n",
      "merging (111, 117) into a new token 271\n",
      "merging (109, 111) into a new token 272\n",
      "merging (115, 101) into a new token 273\n",
      "merging (32, 98) into a new token 274\n",
      "merging (97, 260) into a new token 275\n",
      "merging (97, 103) into a new token 276\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 277\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)  # make a copy\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e3165482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 532\n",
      "ids length: 381\n",
      "compression ratio: 1.40X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\",len(tokens))\n",
    "print(\"ids length:\",len(ids))\n",
    "print(f\"compression ratio: {len(tokens)/len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc514ec",
   "metadata": {},
   "source": [
    "# Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a60282ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for  idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    # give ids (list of integers), return Python String\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6746df67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (256, 111): 257,\n",
       " (32, 97): 258,\n",
       " (101, 110): 259,\n",
       " (110, 100): 260,\n",
       " (256, 104): 261,\n",
       " (115, 32): 262,\n",
       " (105, 110): 263,\n",
       " (116, 32): 264,\n",
       " (97, 114): 265,\n",
       " (258, 32): 266,\n",
       " (101, 108): 267,\n",
       " (101, 120): 268,\n",
       " (257, 107): 269,\n",
       " (269, 259): 270,\n",
       " (111, 117): 271,\n",
       " (109, 111): 272,\n",
       " (115, 101): 273,\n",
       " (32, 98): 274,\n",
       " (97, 260): 275,\n",
       " (97, 103): 276}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ac9ba",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "49946bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # give a string, return list of integers (the tokens)\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break # noting else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "print(encode(\"h\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0e7ddbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you?\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello how are you?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9fbca9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea96a6e5",
   "metadata": {},
   "source": [
    "# Force splits using regex patterns (GPT series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "571851ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', '       ', ' you', '!?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat,\"hello've world123 how's are        you!?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4401a935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'fruits', ' =', ' [\"', 'apple', '\",', ' \"', 'banana', '\",', ' \"', 'cherry', '\"]', '\\n', 'for', ' x', ' in', ' fruits', ':', '\\n ', ' print', '(', 'x', ')', '\\n ', ' if', ' x', ' ==', ' \"', 'banana', '\":', '\\n   ', ' break', '\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
    "for x in fruits:\n",
    "  print(x)\n",
    "  if x == \"banana\":\n",
    "    break\n",
    "\"\"\"\n",
    "\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "374f3572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185]\n",
      "[262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "#GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8accb894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6962e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"-12abcd\"\n",
    "# a=text.encode(\"utf-8\")\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461441a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45, 49, 50, 97, 98, 99, 100]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b6fc979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 114, 101, 32, 109, 97, 114, 107, 101, 100, 32, 98, 121, 32, 97, 32, 226, 138, 155, 32, 105, 110, 32, 116, 104, 101, 32, 110, 97, 109, 101, 32, 97, 110, 100, 32, 111, 117, 116, 108, 105, 110, 101, 100, 32, 105, 109, 97, 103, 101, 115, 59, 32, 116, 104, 101, 105, 114, 32, 105, 109, 97, 103, 101, 115, 32, 109, 97, 121, 32, 115, 104, 111, 119, 32, 97, 115, 32, 97, 32, 103, 114, 111, 117, 112, 32, 119, 105, 116, 104, 32, 226, 128, 156, 226, 128, 166, 226, 128, 157, 32, 98, 101, 102, 111, 114, 101, 32, 97, 110, 100, 32, 97, 102, 116, 101, 114, 46, 32, 79, 112, 101, 110, 65, 73, 39, 115, 32, 108, 97, 114, 103, 101, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 115, 32, 112, 114, 111, 99, 101, 115, 115, 32, 116, 101, 120, 116, 32, 117, 115, 105, 110, 103, 32, 116, 111, 107, 101, 110, 115, 44, 32, 119, 104, 105, 99, 104, 32, 97, 114, 101, 32, 99, 111, 109, 109, 111, 110, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 111, 102, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 102, 111, 117, 110, 100, 32, 105, 110, 32, 97, 32, 115, 101, 116, 32, 111, 102, 32, 116, 101, 120, 116, 46, 32, 84, 104, 101, 32, 109, 111, 100, 101, 108, 115, 32, 108, 101, 97, 114, 110, 32, 116, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 32, 116, 104, 101, 32, 115, 116, 97, 116, 105, 115, 116, 105, 99, 97, 108, 32, 114, 101, 108, 97, 116, 105, 111, 110, 115, 104, 105, 112, 115, 32, 98, 101, 116, 119, 101, 101, 110, 32, 116, 104, 101, 115, 101, 32, 116, 111, 107, 101, 110, 115, 44, 32, 97, 110, 100, 32, 101, 120, 99, 101, 108, 32, 97, 116, 32, 112, 114, 111, 100, 117, 99, 105, 110, 103, 32, 116, 104, 101, 32, 110, 101, 120, 116, 32, 116, 111, 107, 101, 110, 32, 105, 110, 32, 97, 32, 115, 101, 113, 117, 101, 110, 99, 101, 32, 111, 102, 32, 116, 111, 107, 101, 110, 115, 46, 32, 76, 101, 97, 114, 110, 32, 109, 111, 114, 101, 46, 32, 32, 89, 111, 117, 32, 99, 97, 110, 32, 117, 115, 101, 32, 116, 104, 101, 32, 116, 111, 111, 108, 32, 98, 101, 108, 111, 119, 32, 116, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 32, 104, 111, 119, 32, 97, 32, 112, 105, 101, 99, 101, 32, 111, 102, 32, 116, 101, 120, 116, 32, 109, 105, 103, 104, 116, 32, 98, 101, 32, 116, 111, 107, 101, 110, 105, 122, 101, 100, 32, 98, 121, 32, 97, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 44, 32, 97, 110, 100, 32, 116, 104, 101, 32, 116, 111, 116, 97, 108, 32, 99, 111, 117, 110, 116, 32, 111, 102, 32, 116, 111, 107, 101, 110, 115, 32, 105, 110, 32, 116, 104, 97, 116, 32, 112, 105, 101, 99, 101, 32, 111, 102, 32, 116, 101, 120, 116, 46]\n"
     ]
    }
   ],
   "source": [
    "from tokenizer.basic import BasicTokenizer\n",
    "t = BasicTokenizer()\n",
    "text1 = \"aaabdaaabac\"\n",
    "t.train(text1, 256 + 3)\n",
    "print(t.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a72ee511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1st: aaab \n",
      " 2nd: d \n",
      " 3rd: a\n"
     ]
    }
   ],
   "source": [
    "print(f\" 1st: {t.decode([258])} \\n 2nd: {t.decode([100])} \\n 3rd: {t.decode([97])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9cc289d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "compile_pattern = re.compile(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d1fe9898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are',\n",
       " ' marked',\n",
       " ' by',\n",
       " ' a',\n",
       " ' ⊛',\n",
       " ' in',\n",
       " ' the',\n",
       " ' name',\n",
       " ' and',\n",
       " ' outlined',\n",
       " ' images',\n",
       " ';',\n",
       " ' their',\n",
       " ' images',\n",
       " ' may',\n",
       " ' show',\n",
       " ' as',\n",
       " ' a',\n",
       " ' group',\n",
       " ' with',\n",
       " ' “…”',\n",
       " ' before',\n",
       " ' and',\n",
       " ' after',\n",
       " '.',\n",
       " ' OpenAI',\n",
       " \"'s\",\n",
       " ' large',\n",
       " ' language',\n",
       " ' models',\n",
       " ' process',\n",
       " ' text',\n",
       " ' using',\n",
       " ' tokens',\n",
       " ',',\n",
       " ' which',\n",
       " ' are',\n",
       " ' common',\n",
       " ' sequences',\n",
       " ' of',\n",
       " ' characters',\n",
       " ' found',\n",
       " ' in',\n",
       " ' a',\n",
       " ' set',\n",
       " ' of',\n",
       " ' text',\n",
       " '.',\n",
       " ' The',\n",
       " ' models',\n",
       " ' learn',\n",
       " ' to',\n",
       " ' understand',\n",
       " ' the',\n",
       " ' statistical',\n",
       " ' relationships',\n",
       " ' between',\n",
       " ' these',\n",
       " ' tokens',\n",
       " ',',\n",
       " ' and',\n",
       " ' excel',\n",
       " ' at',\n",
       " ' producing',\n",
       " ' the',\n",
       " ' next',\n",
       " ' token',\n",
       " ' in',\n",
       " ' a',\n",
       " ' sequence',\n",
       " ' of',\n",
       " ' tokens',\n",
       " '.',\n",
       " ' Learn',\n",
       " ' more',\n",
       " '.',\n",
       " ' ',\n",
       " ' You',\n",
       " ' can',\n",
       " ' use',\n",
       " ' the',\n",
       " ' tool',\n",
       " ' below',\n",
       " ' to',\n",
       " ' understand',\n",
       " ' how',\n",
       " ' a',\n",
       " ' piece',\n",
       " ' of',\n",
       " ' text',\n",
       " ' might',\n",
       " ' be',\n",
       " ' tokenized',\n",
       " ' by',\n",
       " ' a',\n",
       " ' language',\n",
       " ' model',\n",
       " ',',\n",
       " ' and',\n",
       " ' the',\n",
       " ' total',\n",
       " ' count',\n",
       " ' of',\n",
       " ' tokens',\n",
       " ' in',\n",
       " ' that',\n",
       " ' piece',\n",
       " ' of',\n",
       " ' text',\n",
       " '.']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = re.findall(compile_pattern, text)\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e1826b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.decode([97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2ca9032b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[97, 114, 101],\n",
       " [32, 109, 97, 114, 107, 101, 100],\n",
       " [32, 98, 121],\n",
       " [32, 97],\n",
       " [32, 226, 138, 155],\n",
       " [32, 105, 110],\n",
       " [32, 116, 104, 101],\n",
       " [32, 110, 97, 109, 101],\n",
       " [32, 97, 110, 100],\n",
       " [32, 111, 117, 116, 108, 105, 110, 101, 100],\n",
       " [32, 105, 109, 97, 103, 101, 115],\n",
       " [59],\n",
       " [32, 116, 104, 101, 105, 114],\n",
       " [32, 105, 109, 97, 103, 101, 115],\n",
       " [32, 109, 97, 121],\n",
       " [32, 115, 104, 111, 119],\n",
       " [32, 97, 115],\n",
       " [32, 97],\n",
       " [32, 103, 114, 111, 117, 112],\n",
       " [32, 119, 105, 116, 104],\n",
       " [32, 226, 128, 156, 226, 128, 166, 226, 128, 157],\n",
       " [32, 98, 101, 102, 111, 114, 101],\n",
       " [32, 97, 110, 100],\n",
       " [32, 97, 102, 116, 101, 114],\n",
       " [46],\n",
       " [32, 79, 112, 101, 110, 65, 73],\n",
       " [39, 115],\n",
       " [32, 108, 97, 114, 103, 101],\n",
       " [32, 108, 97, 110, 103, 117, 97, 103, 101],\n",
       " [32, 109, 111, 100, 101, 108, 115],\n",
       " [32, 112, 114, 111, 99, 101, 115, 115],\n",
       " [32, 116, 101, 120, 116],\n",
       " [32, 117, 115, 105, 110, 103],\n",
       " [32, 116, 111, 107, 101, 110, 115],\n",
       " [44],\n",
       " [32, 119, 104, 105, 99, 104],\n",
       " [32, 97, 114, 101],\n",
       " [32, 99, 111, 109, 109, 111, 110],\n",
       " [32, 115, 101, 113, 117, 101, 110, 99, 101, 115],\n",
       " [32, 111, 102],\n",
       " [32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115],\n",
       " [32, 102, 111, 117, 110, 100],\n",
       " [32, 105, 110],\n",
       " [32, 97],\n",
       " [32, 115, 101, 116],\n",
       " [32, 111, 102],\n",
       " [32, 116, 101, 120, 116],\n",
       " [46],\n",
       " [32, 84, 104, 101],\n",
       " [32, 109, 111, 100, 101, 108, 115],\n",
       " [32, 108, 101, 97, 114, 110],\n",
       " [32, 116, 111],\n",
       " [32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100],\n",
       " [32, 116, 104, 101],\n",
       " [32, 115, 116, 97, 116, 105, 115, 116, 105, 99, 97, 108],\n",
       " [32, 114, 101, 108, 97, 116, 105, 111, 110, 115, 104, 105, 112, 115],\n",
       " [32, 98, 101, 116, 119, 101, 101, 110],\n",
       " [32, 116, 104, 101, 115, 101],\n",
       " [32, 116, 111, 107, 101, 110, 115],\n",
       " [44],\n",
       " [32, 97, 110, 100],\n",
       " [32, 101, 120, 99, 101, 108],\n",
       " [32, 97, 116],\n",
       " [32, 112, 114, 111, 100, 117, 99, 105, 110, 103],\n",
       " [32, 116, 104, 101],\n",
       " [32, 110, 101, 120, 116],\n",
       " [32, 116, 111, 107, 101, 110],\n",
       " [32, 105, 110],\n",
       " [32, 97],\n",
       " [32, 115, 101, 113, 117, 101, 110, 99, 101],\n",
       " [32, 111, 102],\n",
       " [32, 116, 111, 107, 101, 110, 115],\n",
       " [46],\n",
       " [32, 76, 101, 97, 114, 110],\n",
       " [32, 109, 111, 114, 101],\n",
       " [46],\n",
       " [32],\n",
       " [32, 89, 111, 117],\n",
       " [32, 99, 97, 110],\n",
       " [32, 117, 115, 101],\n",
       " [32, 116, 104, 101],\n",
       " [32, 116, 111, 111, 108],\n",
       " [32, 98, 101, 108, 111, 119],\n",
       " [32, 116, 111],\n",
       " [32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100],\n",
       " [32, 104, 111, 119],\n",
       " [32, 97],\n",
       " [32, 112, 105, 101, 99, 101],\n",
       " [32, 111, 102],\n",
       " [32, 116, 101, 120, 116],\n",
       " [32, 109, 105, 103, 104, 116],\n",
       " [32, 98, 101],\n",
       " [32, 116, 111, 107, 101, 110, 105, 122, 101, 100],\n",
       " [32, 98, 121],\n",
       " [32, 97],\n",
       " [32, 108, 97, 110, 103, 117, 97, 103, 101],\n",
       " [32, 109, 111, 100, 101, 108],\n",
       " [44],\n",
       " [32, 97, 110, 100],\n",
       " [32, 116, 104, 101],\n",
       " [32, 116, 111, 116, 97, 108],\n",
       " [32, 99, 111, 117, 110, 116],\n",
       " [32, 111, 102],\n",
       " [32, 116, 111, 107, 101, 110, 115],\n",
       " [32, 105, 110],\n",
       " [32, 116, 104, 97, 116],\n",
       " [32, 112, 105, 101, 99, 101],\n",
       " [32, 111, 102],\n",
       " [32, 116, 101, 120, 116],\n",
       " [46]]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids =[list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8534238e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 1/256: (101, 32) -> 256 (b'e ') had 14272 occurrences\n",
      "merge 2/256: (116, 104) -> 257 (b'th') had 9429 occurrences\n",
      "merge 3/256: (46, 10) -> 258 (b'.\\n') had 8887 occurrences\n",
      "merge 4/256: (100, 32) -> 259 (b'd ') had 8193 occurrences\n",
      "merge 5/256: (115, 32) -> 260 (b's ') had 6086 occurrences\n",
      "merge 6/256: (116, 32) -> 261 (b't ') had 6077 occurrences\n",
      "merge 7/256: (105, 110) -> 262 (b'in') had 5341 occurrences\n",
      "merge 8/256: (101, 114) -> 263 (b'er') had 5133 occurrences\n",
      "merge 9/256: (257, 256) -> 264 (b'the ') had 4694 occurrences\n",
      "merge 10/256: (97, 110) -> 265 (b'an') had 4588 occurrences\n",
      "merge 11/256: (121, 32) -> 266 (b'y ') had 3918 occurrences\n",
      "merge 12/256: (111, 117) -> 267 (b'ou') had 3758 occurrences\n",
      "merge 13/256: (111, 32) -> 268 (b'o ') had 3554 occurrences\n",
      "merge 14/256: (101, 259) -> 269 (b'ed ') had 3145 occurrences\n",
      "merge 15/256: (111, 110) -> 270 (b'on') had 3036 occurrences\n",
      "merge 16/256: (101, 110) -> 271 (b'en') had 2917 occurrences\n",
      "merge 17/256: (97, 114) -> 272 (b'ar') had 2426 occurrences\n",
      "merge 18/256: (116, 268) -> 273 (b'to ') had 2326 occurrences\n",
      "merge 19/256: (262, 103) -> 274 (b'ing') had 2255 occurrences\n",
      "merge 20/256: (111, 114) -> 275 (b'or') had 2173 occurrences\n",
      "merge 21/256: (263, 32) -> 276 (b'er ') had 2127 occurrences\n",
      "merge 22/256: (265, 259) -> 277 (b'and ') had 2078 occurrences\n",
      "merge 23/256: (97, 32) -> 278 (b'a ') had 2038 occurrences\n",
      "merge 24/256: (108, 108) -> 279 (b'll') had 1909 occurrences\n",
      "merge 25/256: (102, 32) -> 280 (b'f ') had 1893 occurrences\n",
      "merge 26/256: (104, 97) -> 281 (b'ha') had 1864 occurrences\n",
      "merge 27/256: (114, 101) -> 282 (b're') had 1811 occurrences\n",
      "merge 28/256: (104, 256) -> 283 (b'he ') had 1805 occurrences\n",
      "merge 29/256: (274, 32) -> 284 (b'ing ') had 1770 occurrences\n",
      "merge 30/256: (104, 105) -> 285 (b'hi') had 1700 occurrences\n",
      "merge 31/256: (119, 97) -> 286 (b'wa') had 1623 occurrences\n",
      "merge 32/256: (101, 258) -> 287 (b'e.\\n') had 1543 occurrences\n",
      "merge 33/256: (111, 280) -> 288 (b'of ') had 1505 occurrences\n",
      "merge 34/256: (115, 116) -> 289 (b'st') had 1478 occurrences\n",
      "merge 35/256: (111, 109) -> 290 (b'om') had 1398 occurrences\n",
      "merge 36/256: (115, 258) -> 291 (b's.\\n') had 1309 occurrences\n",
      "merge 37/256: (32, 264) -> 292 (b' the ') had 1294 occurrences\n",
      "merge 38/256: (111, 119) -> 293 (b'ow') had 1241 occurrences\n",
      "merge 39/256: (108, 105) -> 294 (b'li') had 1216 occurrences\n",
      "merge 40/256: (97, 261) -> 295 (b'at ') had 1197 occurrences\n",
      "merge 41/256: (119, 105) -> 296 (b'wi') had 1132 occurrences\n",
      "merge 42/256: (121, 267) -> 297 (b'you') had 1119 occurrences\n",
      "merge 43/256: (116, 105) -> 298 (b'ti') had 1079 occurrences\n",
      "merge 44/256: (279, 32) -> 299 (b'll ') had 1013 occurrences\n",
      "merge 45/256: (100, 258) -> 300 (b'd.\\n') had 999 occurrences\n",
      "merge 46/256: (271, 32) -> 301 (b'en ') had 993 occurrences\n",
      "merge 47/256: (286, 260) -> 302 (b'was ') had 989 occurrences\n",
      "merge 48/256: (257, 101) -> 303 (b'the') had 981 occurrences\n",
      "merge 49/256: (97, 99) -> 304 (b'ac') had 968 occurrences\n",
      "merge 50/256: (114, 105) -> 305 (b'ri') had 955 occurrences\n",
      "merge 51/256: (115, 101) -> 306 (b'se') had 948 occurrences\n",
      "merge 52/256: (262, 32) -> 307 (b'in ') had 929 occurrences\n",
      "merge 53/256: (97, 108) -> 308 (b'al') had 920 occurrences\n",
      "merge 54/256: (102, 275) -> 309 (b'for') had 906 occurrences\n",
      "merge 55/256: (98, 101) -> 310 (b'be') had 902 occurrences\n",
      "merge 56/256: (105, 260) -> 311 (b'is ') had 885 occurrences\n",
      "merge 57/256: (108, 101) -> 312 (b'le') had 880 occurrences\n",
      "merge 58/256: (97, 109) -> 313 (b'am') had 842 occurrences\n",
      "merge 59/256: (110, 111) -> 314 (b'no') had 816 occurrences\n",
      "merge 60/256: (97, 115) -> 315 (b'as') had 809 occurrences\n",
      "merge 61/256: (99, 104) -> 316 (b'ch') had 802 occurrences\n",
      "merge 62/256: (270, 32) -> 317 (b'on ') had 801 occurrences\n",
      "merge 63/256: (103, 104) -> 318 (b'gh') had 799 occurrences\n",
      "merge 64/256: (118, 256) -> 319 (b've ') had 779 occurrences\n",
      "merge 65/256: (108, 259) -> 320 (b'ld ') had 773 occurrences\n",
      "merge 66/256: (97, 116) -> 321 (b'at') had 762 occurrences\n",
      "merge 67/256: (121, 258) -> 322 (b'y.\\n') had 747 occurrences\n",
      "merge 68/256: (105, 114) -> 323 (b'ir') had 729 occurrences\n",
      "merge 69/256: (297, 32) -> 324 (b'you ') had 722 occurrences\n",
      "merge 70/256: (115, 97) -> 325 (b'sa') had 716 occurrences\n",
      "merge 71/256: (105, 116) -> 326 (b'it') had 706 occurrences\n",
      "merge 72/256: (105, 32) -> 327 (b'i ') had 691 occurrences\n",
      "merge 73/256: (263, 256) -> 328 (b'ere ') had 686 occurrences\n",
      "merge 74/256: (111, 111) -> 329 (b'oo') had 684 occurrences\n",
      "merge 75/256: (285, 260) -> 330 (b'his ') had 680 occurrences\n",
      "merge 76/256: (257, 295) -> 331 (b'that ') had 666 occurrences\n",
      "merge 77/256: (108, 266) -> 332 (b'ly ') had 658 occurrences\n",
      "merge 78/256: (104, 276) -> 333 (b'her ') had 653 occurrences\n",
      "merge 79/256: (114, 111) -> 334 (b'ro') had 643 occurrences\n",
      "merge 80/256: (314, 261) -> 335 (b'not ') had 643 occurrences\n",
      "merge 81/256: (107, 32) -> 336 (b'k ') had 641 occurrences\n",
      "merge 82/256: (265, 32) -> 337 (b'an ') had 637 occurrences\n",
      "merge 83/256: (104, 101) -> 338 (b'he') had 632 occurrences\n",
      "merge 84/256: (108, 256) -> 339 (b'le ') had 616 occurrences\n",
      "merge 85/256: (117, 110) -> 340 (b'un') had 612 occurrences\n",
      "merge 86/256: (105, 261) -> 341 (b'it ') had 606 occurrences\n",
      "merge 87/256: (257, 32) -> 342 (b'th ') had 604 occurrences\n",
      "merge 88/256: (100, 105) -> 343 (b'di') had 591 occurrences\n",
      "merge 89/256: (110, 101) -> 344 (b'ne') had 573 occurrences\n",
      "merge 90/256: (119, 104) -> 345 (b'wh') had 558 occurrences\n",
      "merge 91/256: (97, 103) -> 346 (b'ag') had 556 occurrences\n",
      "merge 92/256: (281, 259) -> 347 (b'had ') had 547 occurrences\n",
      "merge 93/256: (100, 101) -> 348 (b'de') had 537 occurrences\n",
      "merge 94/256: (115, 256) -> 349 (b'se ') had 526 occurrences\n",
      "merge 95/256: (293, 32) -> 350 (b'ow ') had 526 occurrences\n",
      "merge 96/256: (116, 258) -> 351 (b't.\\n') had 523 occurrences\n",
      "merge 97/256: (115, 283) -> 352 (b'she ') had 522 occurrences\n",
      "merge 98/256: (115, 104) -> 353 (b'sh') had 517 occurrences\n",
      "merge 99/256: (267, 320) -> 354 (b'ould ') had 511 occurrences\n",
      "merge 100/256: (117, 114) -> 355 (b'ur') had 511 occurrences\n",
      "merge 101/256: (263, 258) -> 356 (b'er.\\n') had 510 occurrences\n",
      "merge 102/256: (101, 260) -> 357 (b'es ') had 506 occurrences\n",
      "merge 103/256: (115, 261) -> 358 (b'st ') had 480 occurrences\n",
      "merge 104/256: (109, 97) -> 359 (b'ma') had 476 occurrences\n",
      "merge 105/256: (309, 32) -> 360 (b'for ') had 469 occurrences\n",
      "merge 106/256: (97, 98) -> 361 (b'ab') had 468 occurrences\n",
      "merge 107/256: (118, 263) -> 362 (b'ver') had 466 occurrences\n",
      "merge 108/256: (115, 105) -> 363 (b'si') had 460 occurrences\n",
      "merge 109/256: (102, 114) -> 364 (b'fr') had 455 occurrences\n",
      "merge 110/256: (296, 342) -> 365 (b'with ') had 443 occurrences\n",
      "merge 111/256: (100, 97) -> 366 (b'da') had 443 occurrences\n",
      "merge 112/256: (116, 116) -> 367 (b'tt') had 437 occurrences\n",
      "merge 113/256: (281, 319) -> 368 (b'have ') had 434 occurrences\n",
      "merge 114/256: (109, 32) -> 369 (b'm ') had 415 occurrences\n",
      "merge 115/256: (303, 266) -> 370 (b'they ') had 415 occurrences\n",
      "merge 116/256: (267, 261) -> 371 (b'out ') had 414 occurrences\n",
      "merge 117/256: (269, 273) -> 372 (b'ed to ') had 409 occurrences\n",
      "merge 118/256: (98, 117) -> 373 (b'bu') had 407 occurrences\n",
      "merge 119/256: (112, 32) -> 374 (b'p ') had 405 occurrences\n",
      "merge 120/256: (288, 264) -> 375 (b'of the ') had 404 occurrences\n",
      "merge 121/256: (105, 99) -> 376 (b'ic') had 398 occurrences\n",
      "merge 122/256: (107, 256) -> 377 (b'ke ') had 397 occurrences\n",
      "merge 123/256: (73, 32) -> 378 (b'I ') had 394 occurrences\n",
      "merge 124/256: (97, 260) -> 379 (b'as ') had 391 occurrences\n",
      "merge 125/256: (114, 32) -> 380 (b'r ') had 385 occurrences\n",
      "merge 126/256: (257, 311) -> 381 (b'this ') had 373 occurrences\n",
      "merge 127/256: (257, 276) -> 382 (b'ther ') had 367 occurrences\n",
      "merge 128/256: (109, 266) -> 383 (b'my ') had 363 occurrences\n",
      "merge 129/256: (290, 256) -> 384 (b'ome ') had 361 occurrences\n",
      "merge 130/256: (316, 32) -> 385 (b'ch ') had 361 occurrences\n",
      "merge 131/256: (108, 32) -> 386 (b'l ') had 361 occurrences\n",
      "merge 132/256: (103, 32) -> 387 (b'g ') had 359 occurrences\n",
      "merge 133/256: (364, 290) -> 388 (b'from') had 358 occurrences\n",
      "merge 134/256: (112, 101) -> 389 (b'pe') had 351 occurrences\n",
      "merge 135/256: (98, 256) -> 390 (b'be ') had 349 occurrences\n",
      "merge 136/256: (270, 256) -> 391 (b'one ') had 348 occurrences\n",
      "merge 137/256: (84, 283) -> 392 (b'The ') had 347 occurrences\n",
      "merge 138/256: (296, 299) -> 393 (b'will ') had 345 occurrences\n",
      "merge 139/256: (102, 101) -> 394 (b'fe') had 339 occurrences\n",
      "merge 140/256: (325, 105) -> 395 (b'sai') had 338 occurrences\n",
      "merge 141/256: (114, 97) -> 396 (b'ra') had 337 occurrences\n",
      "merge 142/256: (109, 256) -> 397 (b'me ') had 337 occurrences\n",
      "merge 143/256: (108, 111) -> 398 (b'lo') had 337 occurrences\n",
      "merge 144/256: (97, 279) -> 399 (b'all') had 329 occurrences\n",
      "merge 145/256: (101, 300) -> 400 (b'ed.\\n') had 323 occurrences\n",
      "merge 146/256: (110, 32) -> 401 (b'n ') had 321 occurrences\n",
      "merge 147/256: (97, 117) -> 402 (b'au') had 314 occurrences\n",
      "merge 148/256: (318, 261) -> 403 (b'ght ') had 313 occurrences\n",
      "merge 149/256: (115, 268) -> 404 (b'so ') had 308 occurrences\n",
      "merge 150/256: (107, 269) -> 405 (b'ked ') had 307 occurrences\n",
      "merge 151/256: (112, 108) -> 406 (b'pl') had 306 occurrences\n",
      "merge 152/256: (271, 261) -> 407 (b'ent ') had 298 occurrences\n",
      "merge 153/256: (267, 110) -> 408 (b'oun') had 298 occurrences\n",
      "merge 154/256: (271, 116) -> 409 (b'ent') had 296 occurrences\n",
      "merge 155/256: (272, 256) -> 410 (b'are ') had 295 occurrences\n",
      "merge 156/256: (112, 111) -> 411 (b'po') had 294 occurrences\n",
      "merge 157/256: (109, 111) -> 412 (b'mo') had 292 occurrences\n",
      "merge 158/256: (119, 328) -> 413 (b'were ') had 292 occurrences\n",
      "merge 159/256: (103, 111) -> 414 (b'go') had 290 occurrences\n",
      "merge 160/256: (115, 117) -> 415 (b'su') had 287 occurrences\n",
      "merge 161/256: (274, 258) -> 416 (b'ing.\\n') had 287 occurrences\n",
      "merge 162/256: (119, 285) -> 417 (b'whi') had 285 occurrences\n",
      "merge 163/256: (323, 32) -> 418 (b'ir ') had 283 occurrences\n",
      "merge 164/256: (395, 259) -> 419 (b'said ') had 282 occurrences\n",
      "merge 165/256: (99, 256) -> 420 (b'ce ') had 281 occurrences\n",
      "merge 166/256: (98, 111) -> 421 (b'bo') had 280 occurrences\n",
      "merge 167/256: (118, 276) -> 422 (b'ver ') had 280 occurrences\n",
      "merge 168/256: (308, 32) -> 423 (b'al ') had 279 occurrences\n",
      "merge 169/256: (116, 276) -> 424 (b'ter ') had 278 occurrences\n",
      "merge 170/256: (273, 264) -> 425 (b'to the ') had 275 occurrences\n",
      "merge 171/256: (116, 111) -> 426 (b'to') had 275 occurrences\n",
      "merge 172/256: (119, 32) -> 427 (b'w ') had 272 occurrences\n",
      "merge 173/256: (116, 101) -> 428 (b'te') had 271 occurrences\n",
      "merge 174/256: (102, 97) -> 429 (b'fa') had 268 occurrences\n",
      "merge 175/256: (373, 261) -> 430 (b'but ') had 266 occurrences\n",
      "merge 176/256: (262, 292) -> 431 (b'in the ') had 265 occurrences\n",
      "merge 177/256: (107, 284) -> 432 (b'king ') had 265 occurrences\n",
      "merge 178/256: (101, 261) -> 433 (b'et ') had 264 occurrences\n",
      "merge 179/256: (112, 112) -> 434 (b'pp') had 259 occurrences\n",
      "merge 180/256: (119, 354) -> 435 (b'would ') had 256 occurrences\n",
      "merge 181/256: (119, 275) -> 436 (b'wor') had 254 occurrences\n",
      "merge 182/256: (102, 117) -> 437 (b'fu') had 254 occurrences\n",
      "merge 183/256: (100, 260) -> 438 (b'ds ') had 253 occurrences\n",
      "merge 184/256: (272, 32) -> 439 (b'ar ') had 253 occurrences\n",
      "merge 185/256: (99, 270) -> 440 (b'con') had 252 occurrences\n",
      "merge 186/256: (98, 266) -> 441 (b'by ') had 252 occurrences\n",
      "merge 187/256: (118, 105) -> 442 (b'vi') had 251 occurrences\n",
      "merge 188/256: (269, 264) -> 443 (b'ed the ') had 249 occurrences\n",
      "merge 189/256: (112, 114) -> 444 (b'pr') had 248 occurrences\n",
      "merge 190/256: (113, 117) -> 445 (b'qu') had 246 occurrences\n",
      "merge 191/256: (258, 264) -> 446 (b'.\\nthe ') had 244 occurrences\n",
      "merge 192/256: (116, 97) -> 447 (b'ta') had 243 occurrences\n",
      "merge 193/256: (119, 101) -> 448 (b'we') had 241 occurrences\n",
      "merge 194/256: (362, 266) -> 449 (b'very ') had 240 occurrences\n",
      "merge 195/256: (388, 32) -> 450 (b'from ') had 237 occurrences\n",
      "merge 196/256: (258, 277) -> 451 (b'.\\nand ') had 231 occurrences\n",
      "merge 197/256: (108, 270) -> 452 (b'lon') had 230 occurrences\n",
      "merge 198/256: (99, 111) -> 453 (b'co') had 229 occurrences\n",
      "merge 199/256: (99, 313) -> 454 (b'cam') had 229 occurrences\n",
      "merge 200/256: (275, 32) -> 455 (b'or ') had 228 occurrences\n",
      "merge 201/256: (97, 299) -> 456 (b'all ') had 228 occurrences\n",
      "merge 202/256: (107, 101) -> 457 (b'ke') had 224 occurrences\n",
      "merge 203/256: (303, 418) -> 458 (b'their ') had 222 occurrences\n",
      "merge 204/256: (99, 101) -> 459 (b'ce') had 221 occurrences\n",
      "merge 205/256: (109, 101) -> 460 (b'me') had 221 occurrences\n",
      "merge 206/256: (115, 260) -> 461 (b'ss ') had 220 occurrences\n",
      "merge 207/256: (100, 111) -> 462 (b'do') had 214 occurrences\n",
      "merge 208/256: (97, 100) -> 463 (b'ad') had 213 occurrences\n",
      "merge 209/256: (109, 105) -> 464 (b'mi') had 213 occurrences\n",
      "merge 210/256: (99, 290) -> 465 (b'com') had 210 occurrences\n",
      "merge 211/256: (72, 256) -> 466 (b'He ') had 207 occurrences\n",
      "merge 212/256: (306, 108) -> 467 (b'sel') had 206 occurrences\n",
      "merge 213/256: (281, 261) -> 468 (b'hat ') had 205 occurrences\n",
      "merge 214/256: (110, 350) -> 469 (b'now ') had 205 occurrences\n",
      "merge 215/256: (117, 112) -> 470 (b'up') had 203 occurrences\n",
      "merge 216/256: (100, 256) -> 471 (b'de ') had 202 occurrences\n",
      "merge 217/256: (270, 258) -> 472 (b'on.\\n') had 201 occurrences\n",
      "merge 218/256: (257, 328) -> 473 (b'there ') had 201 occurrences\n",
      "merge 219/256: (109, 265) -> 474 (b'man') had 199 occurrences\n",
      "merge 220/256: (119, 256) -> 475 (b'we ') had 199 occurrences\n",
      "merge 221/256: (101, 118) -> 476 (b'ev') had 192 occurrences\n",
      "merge 222/256: (110, 268) -> 477 (b'no ') had 192 occurrences\n",
      "merge 223/256: (267, 380) -> 478 (b'our ') had 192 occurrences\n",
      "merge 224/256: (100, 268) -> 479 (b'do ') had 191 occurrences\n",
      "merge 225/256: (102, 105) -> 480 (b'fi') had 190 occurrences\n",
      "merge 226/256: (99, 354) -> 481 (b'could ') had 189 occurrences\n",
      "merge 227/256: (97, 298) -> 482 (b'ati') had 189 occurrences\n",
      "merge 228/256: (111, 112) -> 483 (b'op') had 189 occurrences\n",
      "merge 229/256: (297, 380) -> 484 (b'your ') had 189 occurrences\n",
      "merge 230/256: (109, 287) -> 485 (b'me.\\n') had 188 occurrences\n",
      "merge 231/256: (267, 114) -> 486 (b'our') had 188 occurrences\n",
      "merge 232/256: (265, 103) -> 487 (b'ang') had 187 occurrences\n",
      "merge 233/256: (263, 260) -> 488 (b'ers ') had 186 occurrences\n",
      "merge 234/256: (97, 259) -> 489 (b'ad ') had 185 occurrences\n",
      "merge 235/256: (121, 260) -> 490 (b'ys ') had 185 occurrences\n",
      "merge 236/256: (270, 292) -> 491 (b'on the ') had 185 occurrences\n",
      "merge 237/256: (115, 112) -> 492 (b'sp') had 183 occurrences\n",
      "merge 238/256: (105, 109) -> 493 (b'im') had 182 occurrences\n",
      "merge 239/256: (116, 269) -> 494 (b'ted ') had 182 occurrences\n",
      "merge 240/256: (101, 291) -> 495 (b'es.\\n') had 181 occurrences\n",
      "merge 241/256: (109, 117) -> 496 (b'mu') had 179 occurrences\n",
      "merge 242/256: (114, 117) -> 497 (b'ru') had 178 occurrences\n",
      "merge 243/256: (109, 258) -> 498 (b'm.\\n') had 178 occurrences\n",
      "merge 244/256: (293, 401) -> 499 (b'own ') had 178 occurrences\n",
      "merge 245/256: (98, 108) -> 500 (b'bl') had 178 occurrences\n",
      "merge 246/256: (285, 369) -> 501 (b'him ') had 177 occurrences\n",
      "merge 247/256: (101, 108) -> 502 (b'el') had 177 occurrences\n",
      "merge 248/256: (102, 102) -> 503 (b'ff') had 176 occurrences\n",
      "merge 249/256: (109, 275) -> 504 (b'mor') had 175 occurrences\n",
      "merge 250/256: (106, 117) -> 505 (b'ju') had 174 occurrences\n",
      "merge 251/256: (115, 384) -> 506 (b'some ') had 174 occurrences\n",
      "merge 252/256: (101, 120) -> 507 (b'ex') had 173 occurrences\n",
      "merge 253/256: (112, 117) -> 508 (b'pu') had 172 occurrences\n",
      "merge 254/256: (104, 111) -> 509 (b'ho') had 167 occurrences\n",
      "merge 255/256: (99, 105) -> 510 (b'ci') had 167 occurrences\n",
      "merge 256/256: (361, 371) -> 511 (b'about ') had 166 occurrences\n",
      "[338, 279, 268, 436, 108, 100]\n",
      "merge: 1/256: (104, 101) -> 256 (b'he') had 10639 occurrences\n",
      "merge: 2/256: (32, 116) -> 257 (b' t') had 10294 occurrences\n",
      "merge: 3/256: (46, 10) -> 258 (b'.\\n') had 8887 occurrences\n",
      "merge: 4/256: (32, 97) -> 259 (b' a') had 6324 occurrences\n",
      "merge: 5/256: (105, 110) -> 260 (b'in') had 5341 occurrences\n",
      "merge: 6/256: (257, 256) -> 261 (b' the') had 5042 occurrences\n",
      "merge: 7/256: (32, 115) -> 262 (b' s') had 4712 occurrences\n",
      "merge: 8/256: (32, 119) -> 263 (b' w') had 4584 occurrences\n",
      "merge: 9/256: (114, 101) -> 264 (b're') had 4034 occurrences\n",
      "merge: 10/256: (111, 117) -> 265 (b'ou') had 3758 occurrences\n",
      "merge: 11/256: (32, 104) -> 266 (b' h') had 3283 occurrences\n",
      "merge: 12/256: (101, 100) -> 267 (b'ed') had 3179 occurrences\n",
      "merge: 13/256: (110, 100) -> 268 (b'nd') had 3143 occurrences\n",
      "merge: 14/256: (32, 98) -> 269 (b' b') had 2985 occurrences\n",
      "merge: 15/256: (32, 111) -> 270 (b' o') had 2966 occurrences\n",
      "merge: 16/256: (32, 102) -> 271 (b' f') had 2945 occurrences\n",
      "merge: 17/256: (101, 114) -> 272 (b'er') had 2697 occurrences\n",
      "merge: 18/256: (32, 99) -> 273 (b' c') had 2630 occurrences\n",
      "merge: 19/256: (32, 109) -> 274 (b' m') had 2621 occurrences\n",
      "merge: 20/256: (257, 111) -> 275 (b' to') had 2489 occurrences\n",
      "merge: 21/256: (105, 115) -> 276 (b'is') had 2477 occurrences\n",
      "merge: 22/256: (97, 116) -> 277 (b'at') had 2447 occurrences\n",
      "merge: 23/256: (105, 116) -> 278 (b'it') had 2354 occurrences\n",
      "merge: 24/256: (260, 103) -> 279 (b'ing') had 2255 occurrences\n",
      "merge: 25/256: (101, 110) -> 280 (b'en') had 2137 occurrences\n",
      "merge: 26/256: (111, 110) -> 281 (b'on') had 2109 occurrences\n",
      "merge: 27/256: (32, 100) -> 282 (b' d') had 2053 occurrences\n",
      "merge: 28/256: (97, 115) -> 283 (b'as') had 2038 occurrences\n",
      "merge: 29/256: (32, 112) -> 284 (b' p') had 1964 occurrences\n",
      "merge: 30/256: (108, 108) -> 285 (b'll') had 1909 occurrences\n",
      "merge: 31/256: (32, 108) -> 286 (b' l') had 1844 occurrences\n",
      "merge: 32/256: (97, 114) -> 287 (b'ar') had 1803 occurrences\n",
      "merge: 33/256: (32, 256) -> 288 (b' he') had 1762 occurrences\n",
      "merge: 34/256: (32, 110) -> 289 (b' n') had 1706 occurrences\n",
      "merge: 35/256: (97, 110) -> 290 (b'an') had 1699 occurrences\n",
      "merge: 36/256: (111, 114) -> 291 (b'or') had 1694 occurrences\n",
      "merge: 37/256: (270, 102) -> 292 (b' of') had 1603 occurrences\n",
      "merge: 38/256: (101, 115) -> 293 (b'es') had 1506 occurrences\n",
      "merge: 39/256: (257, 104) -> 294 (b' th') had 1450 occurrences\n",
      "merge: 40/256: (116, 256) -> 295 (b'the') had 1388 occurrences\n",
      "merge: 41/256: (111, 109) -> 296 (b'om') had 1375 occurrences\n",
      "merge: 42/256: (32, 260) -> 297 (b' in') had 1336 occurrences\n",
      "merge: 43/256: (266, 97) -> 298 (b' ha') had 1321 occurrences\n",
      "merge: 44/256: (259, 268) -> 299 (b' and') had 1289 occurrences\n",
      "merge: 45/256: (32, 103) -> 300 (b' g') had 1279 occurrences\n",
      "merge: 46/256: (108, 101) -> 301 (b'le') had 1191 occurrences\n",
      "merge: 47/256: (111, 116) -> 302 (b'ot') had 1155 occurrences\n",
      "merge: 48/256: (121, 265) -> 303 (b'you') had 1119 occurrences\n",
      "merge: 49/256: (111, 119) -> 304 (b'ow') had 1109 occurrences\n",
      "merge: 50/256: (269, 101) -> 305 (b' be') had 1082 occurrences\n",
      "merge: 51/256: (108, 100) -> 306 (b'ld') had 1018 occurrences\n",
      "merge: 52/256: (263, 283) -> 307 (b' was') had 1002 occurrences\n",
      "merge: 53/256: (118, 101) -> 308 (b've') had 999 occurrences\n",
      "merge: 54/256: (32, 101) -> 309 (b' e') had 999 occurrences\n",
      "merge: 55/256: (105, 100) -> 310 (b'id') had 994 occurrences\n",
      "merge: 56/256: (32, 303) -> 311 (b' you') had 986 occurrences\n",
      "merge: 57/256: (97, 268) -> 312 (b'and') had 937 occurrences\n",
      "merge: 58/256: (115, 101) -> 313 (b'se') had 895 occurrences\n",
      "merge: 59/256: (105, 109) -> 314 (b'im') had 887 occurrences\n",
      "merge: 60/256: (97, 121) -> 315 (b'ay') had 868 occurrences\n",
      "merge: 61/256: (97, 99) -> 316 (b'ac') had 862 occurrences\n",
      "merge: 62/256: (108, 121) -> 317 (b'ly') had 854 occurrences\n",
      "merge: 63/256: (105, 99) -> 318 (b'ic') had 841 occurrences\n",
      "merge: 64/256: (97, 108) -> 319 (b'al') had 838 occurrences\n",
      "merge: 65/256: (32, 264) -> 320 (b' re') had 837 occurrences\n",
      "merge: 66/256: (32, 114) -> 321 (b' r') had 823 occurrences\n",
      "merge: 67/256: (288, 114) -> 322 (b' her') had 767 occurrences\n",
      "merge: 68/256: (103, 104) -> 323 (b'gh') had 757 occurrences\n",
      "merge: 69/256: (118, 272) -> 324 (b'ver') had 752 occurrences\n",
      "merge: 70/256: (115, 116) -> 325 (b'st') had 747 occurrences\n",
      "merge: 71/256: (32, 117) -> 326 (b' u') had 733 occurrences\n",
      "merge: 72/256: (270, 110) -> 327 (b' on') had 723 occurrences\n",
      "merge: 73/256: (105, 114) -> 328 (b'ir') had 714 occurrences\n",
      "merge: 74/256: (262, 116) -> 329 (b' st') had 707 occurrences\n",
      "merge: 75/256: (105, 285) -> 330 (b'ill') had 706 occurrences\n",
      "merge: 76/256: (117, 116) -> 331 (b'ut') had 684 occurrences\n",
      "merge: 77/256: (280, 116) -> 332 (b'ent') had 679 occurrences\n",
      "merge: 78/256: (97, 109) -> 333 (b'am') had 677 occurrences\n",
      "merge: 79/256: (107, 101) -> 334 (b'ke') had 673 occurrences\n",
      "merge: 80/256: (97, 100) -> 335 (b'ad') had 659 occurrences\n",
      "merge: 81/256: (289, 302) -> 336 (b' not') had 658 occurrences\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m TokenizerClass, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([BasicTokenizer, RegexTokenizer], [\u001b[33m\"\u001b[39m\u001b[33mbasic\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mregex\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# construct the Tokenizer object and kick off verbose training\u001b[39;00m\n\u001b[32m     16\u001b[39m     tokenizer = TokenizerClass()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     a=tokenizer.encode(\u001b[33m\"\u001b[39m\u001b[33mhello world\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(a)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\LLM\\tokenizer\\regex.py:33\u001b[39m, in \u001b[36mRegexTokenizer.train\u001b[39m\u001b[34m(self, text, vocab_size, verbose)\u001b[39m\n\u001b[32m     30\u001b[39m stats = {}\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk_ids \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# passing in stats will update it in place, addin up counts\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[43mget_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# find the pair with highest count\u001b[39;00m\n\u001b[32m     36\u001b[39m pair = \u001b[38;5;28mmax\u001b[39m(stats, key=stats.get)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\LLM\\tokenizer\\base.py:5\u001b[39m, in \u001b[36mget_stats\u001b[39m\u001b[34m(ids, counts)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_stats\u001b[39m(ids, counts=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      4\u001b[39m     counts = {} \u001b[38;5;28;01mif\u001b[39;00m counts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m counts\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      6\u001b[39m         counts[pair] = counts.get(pair, \u001b[32m0\u001b[39m) + \u001b[32m1\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m counts\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from tokenizer.basic import BasicTokenizer\n",
    "from tokenizer.regex import RegexTokenizer\n",
    "\n",
    "# open some text and train a vocab of 512 tokens\n",
    "text = open(\"transcripts_only.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "# create a directory for models, so we don't pollute the current directory\n",
    "# os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "t0 = time.time()\n",
    "for TokenizerClass, name in zip([BasicTokenizer, RegexTokenizer], [\"basic\", \"regex\"]):\n",
    "\n",
    "    # construct the Tokenizer object and kick off verbose training\n",
    "    tokenizer = TokenizerClass()\n",
    "    tokenizer.train(text, 512, verbose=True)\n",
    "    a=tokenizer.encode(\"hello world\")\n",
    "    print(a)\n",
    "    # writes two files in the models directory: name.model, and name.vocab\n",
    "    # prefix = os.path.join(\"models\", name)\n",
    "    tokenizer.save(name)\n",
    "t1 = time.time()\n",
    "\n",
    "print(f\"Training took {t1 - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46f2f5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 101, 301, 111, 32, 119, 291, 108, 100]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.load(\"basic.model\")\n",
    "tokenizer.encode(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8abfb2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[263, 301, 111, 340, 258, 509]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.load(\"regex.model\")\n",
    "tokenizer.encode(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb4a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6259e700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_ID = max(tokenizer.vocab.keys()) + 1\n",
    "print(PAD_ID)\n",
    "tokenizer.vocab[PAD_ID] = b\"\"   # empty byte\n",
    "tokenizer.decode([514])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
